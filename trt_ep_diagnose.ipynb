{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jetson ORT TensorRT EP Diagnose\n",
        "\n",
        "This notebook helps you find why TensorRT EP is not active on Jetson.\n",
        "It collects environment info, verifies TensorRT libraries, and tries to create an ONNX Runtime session\n",
        "with detailed logging.\n",
        "\n",
        "**Tip:** If you already imported `onnxruntime` in this kernel, restart the kernel and run from the top\n",
        "so logging settings take effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "print('Python:', sys.version.replace('', ' '))\n",
        "print('Platform:', platform.platform())\n",
        "print('Machine:', platform.machine())\n",
        "print('CWD:', Path.cwd())\n",
        "\n",
        "env_keys = [\n",
        "    'LD_LIBRARY_PATH',\n",
        "    'PATH',\n",
        "    'CUDA_VISIBLE_DEVICES',\n",
        "    'ORT_LOG_SEVERITY_LEVEL',\n",
        "    'ORT_LOG_VERBOSITY_LEVEL',\n",
        "]\n",
        "for key in env_keys:\n",
        "    value = os.environ.get(key, '')\n",
        "    print(f'{key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: check Python packages and their versions\n",
        "def _try_import(name):\n",
        "    try:\n",
        "        module = __import__(name)\n",
        "        version = getattr(module, '__version__', 'unknown')\n",
        "        print(f'{name} version:', version)\n",
        "        return module\n",
        "    except Exception as exc:\n",
        "        print(f'{name} import failed:', exc)\n",
        "        return None\n",
        "\n",
        "_try_import('onnxruntime')\n",
        "_try_import('tensorrt')\n",
        "_try_import('torch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check TensorRT shared libraries\n",
        "import ctypes\n",
        "\n",
        "libs = [\n",
        "    'libnvinfer.so',\n",
        "    'libnvinfer_plugin.so',\n",
        "    'libnvonnxparser.so',\n",
        "]\n",
        "for lib in libs:\n",
        "    try:\n",
        "        ctypes.CDLL(lib)\n",
        "        print(lib, 'OK')\n",
        "    except OSError as exc:\n",
        "        print(lib, 'FAIL', exc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set ORT logging and import onnxruntime\n",
        "If you already imported onnxruntime in this kernel, restart and run from the top.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if 'onnxruntime' in sys.modules:\n",
        "    print('onnxruntime already imported. Restart the kernel and run from the top for logging to work.')\n",
        "else:\n",
        "    os.environ['ORT_LOG_SEVERITY_LEVEL'] = '0'\n",
        "    os.environ['ORT_LOG_VERBOSITY_LEVEL'] = '1'\n",
        "    import onnxruntime as ort\n",
        "    print('ORT version:', ort.__version__)\n",
        "    print('Available providers:', ort.get_available_providers())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model path and input shape\n",
        "Edit the model path if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_path = Path('yolo26n_my.onnx')\n",
        "print('Model exists:', model_path.exists(), model_path.resolve())\n",
        "\n",
        "if model_path.exists():\n",
        "    import onnxruntime as ort\n",
        "    sess = ort.InferenceSession(str(model_path), providers=['CUDAExecutionProvider'])\n",
        "    inp = sess.get_inputs()[0]\n",
        "    print('Input name:', inp.name)\n",
        "    print('Input shape:', inp.shape)\n",
        "    print('Input type:', getattr(inp, 'type', None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try TensorRT EP with verbose logging\n",
        "This should show any TensorRT initialization failures in the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "trt_cache = Path('trt_cache')\n",
        "trt_cache.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "providers = [\n",
        "    (\n",
        "        'TensorrtExecutionProvider',\n",
        "        {\n",
        "            'trt_fp16_enable': True,\n",
        "            'trt_engine_cache_enable': True,\n",
        "            'trt_engine_cache_path': str(trt_cache),\n",
        "            'trt_max_workspace_size': 2 * 1024 * 1024 * 1024,\n",
        "        },\n",
        "    ),\n",
        "    ('CUDAExecutionProvider', {'device_id': 0}),\n",
        "]\n",
        "\n",
        "so = ort.SessionOptions()\n",
        "so.log_severity_level = 0\n",
        "so.log_verbosity_level = 1\n",
        "\n",
        "try:\n",
        "    sess = ort.InferenceSession(str(model_path), sess_options=so, providers=providers)\n",
        "    print('Actual providers:', sess.get_providers())\n",
        "except Exception as exc:\n",
        "    print('Session creation failed:', exc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUDA-only baseline (for comparison)\n",
        "If TRT fails, this confirms CUDA EP works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "sess = ort.InferenceSession(str(model_path), providers=['CUDAExecutionProvider'])\n",
        "inp = sess.get_inputs()[0]\n",
        "shape = [d if isinstance(d, int) else 1 for d in inp.shape]\n",
        "dummy = np.random.rand(*shape).astype(np.float32)\n",
        "\n",
        "# warmup\n",
        "for _ in range(5):\n",
        "    sess.run(None, {inp.name: dummy})\n",
        "\n",
        "iters = 50\n",
        "start = time.perf_counter()\n",
        "for _ in range(iters):\n",
        "    sess.run(None, {inp.name: dummy})\n",
        "end = time.perf_counter()\n",
        "avg_ms = (end - start) / iters * 1000\n",
        "print(f'CUDA EP avg latency: {avg_ms:.3f} ms')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "- If TRT EP is available but not active, the logs above will usually show missing libraries or version mismatch.\n",
        "- If CUDA EP works and TRT EP fails, the issue is specific to TensorRT integration (not the model).\n",
        "- If both fail, the ONNX model or ORT install is broken.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
